# ğŸ² FarkleAI

Un projet d'intelligence artificielle utilisant l'apprentissage par renforcement (Deep Q-Network) pour jouer au jeu de dÃ©s Farkle.

## ğŸ“‹ Description

FarkleAI est une implÃ©mentation d'un agent d'intelligence artificielle qui apprend Ã  jouer au jeu Farkle en utilisant l'algorithme Deep Q-Network (DQN) avec des amÃ©liorations avancÃ©es comme l'Ã©chantillonnage prioritaire d'expÃ©riences (Prioritized Experience Replay).

Le jeu Farkle est un jeu de dÃ©s oÃ¹ les joueurs tentent d'accumuler des points en sÃ©lectionnant des combinaisons de dÃ©s valides, tout en risquant de perdre leurs points s'ils continuent Ã  jouer et obtiennent un "Farkle" (aucun dÃ© valide).

## ğŸ¯ FonctionnalitÃ©s

- **Agent DQN avancÃ©** : ImplÃ©mentation d'un rÃ©seau de neurones profond pour l'apprentissage par renforcement
- **Ã‰chantillonnage prioritaire** : Utilisation de Prioritized Experience Replay pour amÃ©liorer l'efficacitÃ© de l'apprentissage
- **Environnement de jeu complet** : Simulation complÃ¨te du jeu Farkle avec toutes ses rÃ¨gles
- **Ã‰valuation des performances** : Outils pour Ã©valuer et analyser les performances de l'agent
- **Visualisation** : Graphiques de progression et d'analyse des stratÃ©gies

## ğŸ—ï¸ Architecture du projet

```
FarkleAI-master/
â”œâ”€â”€ algorithm/
â”‚   â””â”€â”€ dqn.py              # ImplÃ©mentation de l'algorithme DQN
â”œâ”€â”€ models/
â”‚   â””â”€â”€ FarkleAI_DQN_0.h5   # ModÃ¨le prÃ©-entraÃ®nÃ©
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ farkle_model.h5     # ModÃ¨le sauvegardÃ©
â”œâ”€â”€ farkle.py               # Environnement de jeu Farkle
â”œâ”€â”€ main.py                 # Script principal d'entraÃ®nement et d'Ã©valuation
â”œâ”€â”€ train.py                # Script d'entraÃ®nement simplifiÃ©
â”œâ”€â”€ evaluate.py             # Script d'Ã©valuation
â””â”€â”€ requirement.txt         # DÃ©pendances Python
```

## ğŸš€ Installation

1. **Cloner le repository**
   ```bash
   git clone https://github.com/eddys999/FarkleAI.git
   cd FarkleAI
   ```

2. **Installer les dÃ©pendances**
   ```bash
   pip install -r requirement.txt
   ```

## ğŸ® Utilisation

### EntraÃ®nement de l'agent

```bash
# EntraÃ®nement simple
python train.py

# EntraÃ®nement avec paramÃ¨tres personnalisÃ©s
python main.py
```

### Ã‰valuation du modÃ¨le

```bash
# Ã‰valuer un modÃ¨le prÃ©-entraÃ®nÃ©
python main.py
```

### ParamÃ¨tres configurables

- `nb_episodes` : Nombre d'Ã©pisodes d'entraÃ®nement (dÃ©faut: 5000)
- `learning_rate` : Taux d'apprentissage (dÃ©faut: 0.0001)
- `epsilon` : ParamÃ¨tre d'exploration (dÃ©faut: 1.0 â†’ 0.1)
- `gamma` : Facteur de remise (dÃ©faut: 0.95)

## ğŸ§  Algorithme DQN

L'agent utilise un rÃ©seau de neurones profond avec les caractÃ©ristiques suivantes :

- **Architecture** : 3 couches denses (64, 512, 512 neurones)
- **Fonction d'activation** : ReLU
- **Optimiseur** : Adam avec taux d'apprentissage adaptatif
- **Fonction de perte** : Huber Loss
- **MÃ©moire** : Buffer de 2000 expÃ©riences avec Ã©chantillonnage prioritaire

### AmÃ©liorations avancÃ©es

- **Prioritized Experience Replay (PER)** : Les expÃ©riences importantes sont Ã©chantillonnÃ©es plus frÃ©quemment
- **Target Network** : RÃ©seau cible pour stabiliser l'apprentissage
- **Epsilon Decay** : Exploration dÃ©croissante au cours de l'entraÃ®nement

## ğŸ² RÃ¨gles du jeu Farkle

Le jeu Farkle implÃ©mentÃ© suit ces rÃ¨gles :

- **Objectif** : Atteindre 5000 points
- **DÃ©s** : 6 dÃ©s par tour
- **Combinaisons valides** :
  - 1 = 100 points
  - 5 = 50 points
  - Triplets = 100 Ã— valeur du dÃ© (1 = 1000)
  - Suite complÃ¨te (1-6) = 1500 points
  - Trois paires = 1500 points
  - Deux triplets = 2500 points

- **Risque** : Si aucun dÃ© valide n'est obtenu, le joueur perd tous les points du tour (Farkle)

## ğŸ“Š MÃ©triques de performance

L'agent est Ã©valuÃ© sur :

- **Taux de victoire** : Pourcentage de parties gagnÃ©es
- **Score moyen** : Score moyen par partie
- **Nombre de Farkles** : FrÃ©quence des Ã©checs
- **Nombre d'Ã©tapes** : EfficacitÃ© du jeu

## ğŸ”§ DÃ©pendances principales

- `tensorflow` : Framework de deep learning
- `numpy` : Calculs numÃ©riques
- `matplotlib` : Visualisation
- `tqdm` : Barres de progression
- `psutil` : Monitoring systÃ¨me

## ğŸ“ˆ RÃ©sultats

L'agent entraÃ®nÃ© dÃ©montre :

- Une stratÃ©gie d'Ã©quilibre entre risque et rÃ©compense
- Une capacitÃ© d'apprentissage des combinaisons optimales
- Une adaptation aux diffÃ©rents Ã©tats du jeu

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

*Pour toute question ou suggestion, n'hÃ©sitez pas Ã  ouvrir une issue sur GitHub !*
